{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ceb9fc4",
   "metadata": {},
   "source": [
    "# üìâ Linear Regression from Scratch ‚Äî Predicting House Prices\n",
    "\n",
    "This notebook demonstrates how **Linear Regression** works using **NumPy**, without relying on any machine learning libraries.  \n",
    "We‚Äôll build a simple model to predict house prices based on their size (in square feet).\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objectives\n",
    "- Understand the concept of linear regression.\n",
    "- Implement gradient descent manually.\n",
    "- Visualize the regression line and loss reduction.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d538acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data: (Size in square feet, Price in $1000)\n",
    "X = np.array([650, 785, 900, 1100, 1200, 1400, 1500])\n",
    "y = np.array([100, 120, 130, 150, 170, 180, 195])\n",
    "\n",
    "plt.scatter(X, y, color='blue', label='Data Points')\n",
    "plt.title(\"House Price vs Size\")\n",
    "plt.xlabel(\"Size (sq ft)\")\n",
    "plt.ylabel(\"Price ($1000s)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4652e1",
   "metadata": {},
   "source": [
    "## üßÆ Step 1: Normalize the data\n",
    "Normalization helps gradient descent converge faster by scaling down large values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59778a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean, X_std = np.mean(X), np.std(X)\n",
    "y_mean, y_std = np.mean(y), np.std(y)\n",
    "\n",
    "X_norm = (X - X_mean) / X_std\n",
    "y_norm = (y - y_mean) / y_std\n",
    "\n",
    "plt.scatter(X_norm, y_norm, color='orange')\n",
    "plt.title(\"Normalized Data\")\n",
    "plt.xlabel(\"Size (normalized)\")\n",
    "plt.ylabel(\"Price (normalized)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5a76e8",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 2: Initialize parameters\n",
    "We'll initialize weights randomly and use **gradient descent** to minimize loss (Mean Squared Error).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263532b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "m = 0.0  # slope\n",
    "b = 0.0  # intercept\n",
    "lr = 0.1  # learning rate\n",
    "epochs = 100\n",
    "\n",
    "# Store losses for visualization\n",
    "losses = []\n",
    "\n",
    "# Gradient Descent Loop\n",
    "for i in range(epochs):\n",
    "    y_pred = m * X_norm + b\n",
    "    error = y_pred - y_norm\n",
    "\n",
    "    dm = (2 / len(X_norm)) * np.dot(error, X_norm)\n",
    "    db = (2 / len(X_norm)) * np.sum(error)\n",
    "\n",
    "    m -= lr * dm\n",
    "    b -= lr * db\n",
    "\n",
    "    loss = np.mean(error ** 2)\n",
    "    losses.append(loss)\n",
    "\n",
    "print(f\"Trained parameters: slope={m:.3f}, intercept={b:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7c4930",
   "metadata": {},
   "source": [
    "## üìâ Step 3: Visualize loss during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac5e95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses, color='purple')\n",
    "plt.title(\"Loss Reduction Over Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593172f2",
   "metadata": {},
   "source": [
    "## üßæ Step 4: Visualize regression line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1c4022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denormalize the line for plotting\n",
    "m_real = m * (y_std / X_std)\n",
    "b_real = y_mean + y_std * (b - m * X_mean / X_std)\n",
    "\n",
    "x_line = np.linspace(min(X), max(X), 100)\n",
    "y_line = m_real * x_line + b_real\n",
    "\n",
    "plt.scatter(X, y, color='blue', label='Data Points')\n",
    "plt.plot(x_line, y_line, color='red', label='Regression Line')\n",
    "plt.title(\"Linear Regression: House Price Prediction\")\n",
    "plt.xlabel(\"Size (sq ft)\")\n",
    "plt.ylabel(\"Price ($1000s)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb22f47",
   "metadata": {},
   "source": [
    "## üßÆ Step 5: Make predictions\n",
    "Let's predict the price for a new house size, e.g., **1300 sq ft**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955a82ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_house = 1300\n",
    "predicted_price = m_real * new_house + b_real\n",
    "print(f\"Predicted Price for {new_house} sq ft house: ${predicted_price*1000:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703243d0",
   "metadata": {},
   "source": [
    "---\n",
    "### ‚úÖ Summary\n",
    "In this notebook, you learned:\n",
    "- How linear regression fits a straight line to data.\n",
    "- How gradient descent adjusts parameters to minimize error.\n",
    "- How to visualize loss reduction and regression lines.\n",
    "---"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
